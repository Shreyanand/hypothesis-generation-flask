{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shrey/AnacondaProjects/stanford-corenlp-full-2018-02-27\n",
      "Core NLP instance\n",
      "Word 2 Vec model ready\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "app = Flask(__name__)\n",
    "from nltk.corpus import wordnet\n",
    "import pymysql\n",
    "import configparser\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.txt\")\n",
    "word2vec_path = config.get(\"configuration\",\"word2vec_path\")\n",
    "stanford_corenlp_path = config.get(\"configuration\",\"stanford_corenlp_path\")\n",
    "print(stanford_corenlp_path)\n",
    "\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "# Connect to DB\n",
    "mydbhost = config.get(\"configuration\",\"mydbhost\")\n",
    "mydbuser = config.get(\"configuration\",\"mydbuser\")\n",
    "mydbpasswd = config.get(\"configuration\",\"mydbpasswd\")\n",
    "mydbdb = config.get(\"configuration\",\"mydbdb\")\n",
    "\n",
    "nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "print('Core NLP instance')\n",
    "# Importing word2vec to find similarity and neighboring words\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000) \n",
    "print('Word 2 Vec model ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(word, nlp):\n",
    "    \"\"\" This function calls Stanford Core Nlp's sentiment annotator\n",
    "    \n",
    "        Args:\n",
    "            word (String): The input word for setiment detection\n",
    "            nlp (StanfordCoreNLP): Core nlp instance for sentiment detection\n",
    "        Returns:\n",
    "            String : Sentiment value: Very negative; Negative; Neutral; \n",
    "            postive; Very positive\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sjson = nlp.annotate(word,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000})\n",
    "        res = json.loads(sjson)\n",
    "        sval = res[\"sentences\"][0][\"sentiment\"]\n",
    "        return(sval)\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ners(sentence, nlp):\n",
    "    sentence_ner = nlp.ner(sentence + ' dummy')\n",
    "    to_replace_ners = []\n",
    "    org = ''\n",
    "    kind = ''\n",
    "    for (i, j) in sentence_ner: #i = NCSU, j = ORGANISATION; i = bent, j = 0\n",
    "      if(j == 'O'):\n",
    "        if org != '': \n",
    "            to_replace_ners.append((org[1:], kind))\n",
    "            org = ''\n",
    "        else:\n",
    "            pass\n",
    "      else:\n",
    "        org = org + \"_\" + i\n",
    "        kind = j\n",
    "    #print (sentence_ner)   \n",
    "    return to_replace_ners\n",
    "\n",
    "#ners(\"Barrack Obama went to the Harvard University\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tags(statement, nlp):\n",
    "    \n",
    "    sentence_tags = nlp.pos_tag(statement)\n",
    "    \n",
    "    to_replace_verbs = []\n",
    "    to_replace_verbphrases = []\n",
    "    to_replace_adjectives = []\n",
    "    to_replace_adjphrases = []\n",
    "    to_replace_nouns = []\n",
    "    to_replace_nounphrases = []\n",
    "    \n",
    "    verb_check = 0\n",
    "    noun_check = 0\n",
    "    adj_check = 0\n",
    "\n",
    "    for (i, j) in sentence_tags: # Here verb checks are activated after a verb is discovered, so pair of verbs as verbphrases? Check Logic.\n",
    "        \n",
    "        \n",
    "      if(verb_check == 1):\n",
    "        verbphrase = verb + '_' + i\n",
    "        to_replace_verbphrases.append((verbphrase, i))\n",
    "        verb_check = 0 \n",
    "\n",
    "      if(noun_check == 1):\n",
    "        nounphrase = noun + '_' + i\n",
    "        to_replace_nounphrases.append((nounphrase, i))\n",
    "        noun_check = 0 \n",
    "\n",
    "      if(adj_check == 1):\n",
    "        adjphrase = adj + '_' + i\n",
    "        to_replace_adjphrases.append((adjphrase, i))\n",
    "        adj_check = 0 \n",
    "\n",
    "      if(j == 'VBD' or j=='VBZ' or j == 'VBP' or j == 'VBN' or j == 'VBG'):\n",
    "        to_replace_verbs.append(i)\n",
    "        verb = i\n",
    "        verb_check = 1\n",
    "\n",
    "      if(j == 'NN' or j=='NNS' or j == 'NNP' or j == 'NNPS'):\n",
    "        to_replace_nouns.append(i)\n",
    "        noun = i\n",
    "        noun_check = 1\n",
    "\n",
    "      if(j == 'JJ'):\n",
    "        to_replace_adjectives.append(i)\n",
    "        adj = i\n",
    "        adj_check = 1\n",
    "        \n",
    "    return (to_replace_verbs, \n",
    "            to_replace_verbphrases,\n",
    "            to_replace_adjectives,\n",
    "            to_replace_adjphrases,\n",
    "            to_replace_nouns,\n",
    "            to_replace_nounphrases)\n",
    "\n",
    "pos_tags(\"Barrack Obama went to the Harvard University\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_similar(to_replace_ners, to_replace_verbs, to_replace_verbphrases,\n",
    "    to_replace_adjectives, to_replace_adjphrases,\n",
    "    to_replace_nouns, to_replace_nounphrases,  model):\n",
    "    \n",
    "    ## These are using W2V, currently used.\n",
    "    topk = 10\n",
    "    replacement_ners = []\n",
    "    replacement_verbs = []\n",
    "    replacement_verbphrases = []\n",
    "    replacement_nouns = []\n",
    "    replacement_nounphrases = []\n",
    "    replacement_adjectives = []\n",
    "    replacement_adjphrases = []\n",
    "    \n",
    "    for (i, j) in to_replace_ners:\n",
    "        try:\n",
    "            similar_ners = model.most_similar([i, j.lower()], [], topk)\n",
    "            senti_ners = []\n",
    "            for (similar_ner, score) in similar_ners:\n",
    "                senti_ners.append([similar_ner, score, sentiments(similar_ner, nlp)])\n",
    "            replacement_ners.append((i, senti_ners))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "        \n",
    "        ## Use W2V for replacing nouns, verbs, adj verbphrases, nounphrases, and adj phrases\n",
    "    \n",
    "    for verb in to_replace_verbs:\n",
    "        try: \n",
    "            similar_verbs = model.most_similar(verb, [], topk)\n",
    "            senti_verbs = [[i, j, sentiments(i, nlp)] for (i, j) in similar_verbs]\n",
    "            replacement_verbs.append((verb, senti_verbs))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            \n",
    "    for (verbphrase, nn) in to_replace_verbphrases:\n",
    "        try:\n",
    "            similar_verbphrases = model.most_similar([verbphrase, nn], [], topk)\n",
    "            replacement_verbphrases.append((verbphrase, similar_verbphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    " \n",
    "    for noun in to_replace_nouns:\n",
    "        try:\n",
    "            similar_nouns = model.most_similar(noun, [], topk)\n",
    "            senti_nouns = [[i, j, sentiments(i, nlp)] for (i, j) in similar_nouns]\n",
    "            replacement_nouns.append((noun, senti_nouns))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for (nounphrase, nn) in to_replace_nounphrases:\n",
    "        try:\n",
    "            similar_nounphrases = model.most_similar([nounphrase, nn], [], topk)\n",
    "            replacement_nounphrases.append((nounphrase, similar_nounphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for adjective in to_replace_adjectives: \n",
    "        try: \n",
    "            similar_adjectives = model.most_similar(adjective, [], topk)\n",
    "            senti_adjectives = [[i, j, sentiments(i, nlp)] for (i, j) in similar_adjectives]\n",
    "            replacement_adjectives.append((adjective, senti_adjectives))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "        \n",
    "    \n",
    "    \n",
    "    for (adjphrase, nn) in to_replace_adjphrases:\n",
    "        try:\n",
    "            similar_adjphrases = model.most_similar([adjphrase, nn], [], topk)\n",
    "            replacement_adjphrases.append((adjphrase, similar_adjphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "#     print(\"Alternative NERs\" + str(replacement_ners))\n",
    "#     print(\"Alternative Verbs\" +  str(replacement_verbs))\n",
    "#     print(\"Alternative Verb Phrases\" + str(replacement_verbphrases))\n",
    "#     print(\"Alternative Noun\" + str(replacement_nouns))\n",
    "#     print(\"Alternative Noun Phrases\" + str(replacement_nounphrases))\n",
    "#     print(\"Alternative Adjectives\" + str(replacement_adjectives))\n",
    "#     print(\"Alternative Adjective Phrases\" + str(replacement_adjphrases))\n",
    "    \n",
    "    \n",
    "    return (replacement_ners,\n",
    "            replacement_verbs,\n",
    "            replacement_verbphrases,\n",
    "            replacement_nouns,\n",
    "            replacement_nounphrases,\n",
    "            replacement_adjectives,\n",
    "            replacement_adjphrases)\n",
    "\n",
    "# statement = \"Barrack Obama went to the Harvard University\" \n",
    "# to_replace_verbs, to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases, to_replace_nouns, to_replace_nounphrases = pos_tags(\n",
    "#     statement, nlp)\n",
    "# w2v_similar(ners(statement, nlp), to_replace_verbs, \n",
    "#             to_replace_verbphrases,\n",
    "#             to_replace_adjectives,\n",
    "#             to_replace_adjphrases,\n",
    "#             to_replace_nouns,\n",
    "#             to_replace_nounphrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barack_Obama', 'PERSON'), ('Harvard_University', 'ORGANIZATION')]\n",
      "Barack_Obama\n",
      "Harvard_University\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Barack_Obama', '2008_elections_in_the_United_States>'),\n",
       " ('Barack_Obama', 'United_States_Senators_from_Illinois>'),\n",
       " ('Barack_Obama', 'African-American_non-fiction_writers>'),\n",
       " ('Barack_Obama', 'American_diarists>'),\n",
       " ('Barack_Obama', \"Public_boys'_schools_in_the_United_States>\"),\n",
       " ('Barack_Obama', 'American_jurists>'),\n",
       " ('Barack_Obama', 'Terrorist_incidents_in_the_United_States_in_2008>'),\n",
       " ('Barack_Obama', '20th-century_American_writers>'),\n",
       " ('Barack_Obama', 'Educational_institutions_established_in_2011>'),\n",
       " ('Barack_Obama', 'Crimes_in_Tennessee>'),\n",
       " ('Harvard_University', 'Botany_journals>'),\n",
       " ('Harvard_University', 'Harvard_Freshman_Dormitories>'),\n",
       " ('Harvard_University',\n",
       "  'Private_universities_and_colleges_in_the_United_States>'),\n",
       " ('Harvard_University', 'School_buildings_completed_in_1815>'),\n",
       " ('Harvard_University', 'National_Historic_Landmarks_in_Massachusetts>'),\n",
       " ('Harvard_University', 'Universities_and_colleges_in_the_United_States>'),\n",
       " ('Harvard_University', 'Harvard_University_buildings>'),\n",
       " ('Harvard_University', 'Academic_journals_published_in_the_United_States>'),\n",
       " ('Harvard_University', 'Churches_in_Cambridge,_Massachusetts>'),\n",
       " ('Harvard_University', 'Harvard_University_buildings>')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect to Wikipedia Database of instance and class. \n",
    "## For each ner, find similar ners using W2V and append to replacement_ners (earlier version)\n",
    "## For each similar ner, find it's category from simple types Wikipedia Database\n",
    "def wiki_ontology(to_replace_ners):\n",
    "    #print(to_replace_ners)\n",
    "    ner_categories = []\n",
    "    \n",
    "    for ner in to_replace_ners:\n",
    "        replace = []\n",
    "        connection = pymysql.connect(host=mydbhost, user=mydbuser, passwd=mydbpasswd, db=mydbdb)\n",
    "        ne = ner[0]\n",
    "        print(ne)\n",
    "        try:\n",
    "          with connection.cursor() as cursor:\n",
    "            # Execute SQL select statement\n",
    "           query = \"SELECT instance, class FROM simple_types where instance like CONCAT('%', '\"+ ne + \"' ,'%')\"\n",
    "           cursor.execute(query)\n",
    "           # Get the number of rows in the resultset\n",
    "           numrows = cursor.rowcount\n",
    "           # Get and display one row at a time\n",
    "           for x in range(0, 10): #top 10 results from wikicat\n",
    "             row = cursor.fetchone()\n",
    "             ner_categories.append((ne, row[1][9:]))\n",
    "        finally:\n",
    "          # Close connection.\n",
    "          connection.close()\n",
    "                    \n",
    "    return (ner_categories)\n",
    "\n",
    "# nlp = StanfordCoreNLP(stanford_corenlp_path)\n",
    "# print(\"core nlp\")\n",
    "to_replace_nlp = ners(\"Barack Obama went to the Harvard University\", nlp)\n",
    "print(to_replace_nlp)\n",
    "wiki_ontology(to_replace_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/\")\n",
    "def main():\n",
    "  print('Main Page opened')\n",
    "  return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/explore',methods=['POST'])\n",
    "def explore():   \n",
    "    # importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "    # Tree syntax of natural language: http://www.cs.cornell.edu/courses/cs474/2004fa/lec1.pdf\n",
    "\n",
    "\n",
    "    # print(\"In Explore\")\n",
    "    # print(\"StanfordCoreNLP path:\", stanford_corenlp_path)\n",
    "    \n",
    "\n",
    "    # read the posted values from the UI\n",
    "    statement = ''\n",
    "    print('explore')\n",
    "    if request.form.get('inputStatement', None):\n",
    "        statement = request.form['inputStatement']\n",
    "        print(statement)\n",
    "    \n",
    "    \n",
    "    statement_sentiment = \"Sentiment of the query is: \" + sentiments(statement, nlp)\n",
    "    to_replace_ners = ners(statement, nlp)\n",
    "    print(statement_sentiment)\n",
    "    #sentence_tokens = nlp.word_tokenize(statement)\n",
    "    # sentence_parse = nlp.parse(statement)\n",
    "    # sentence_dependency = nlp.dependency_parse(statement)\n",
    "\n",
    "    to_replace_verbs, to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases, to_replace_nouns, to_replace_nounphrases = pos_tags(statement, nlp)\n",
    "    \n",
    "    replacement_ners, replacement_verbs, replacement_verbphrases, replacement_nouns, replacement_nounphrases, replacement_adjectives, replacement_adjphrases = w2v_similar(\n",
    "        to_replace_ners, to_replace_verbs, \n",
    "        to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases,\n",
    "        to_replace_nouns, to_replace_nounphrases, model)\n",
    "\n",
    "    print(\"Analysis complete\")\n",
    "    return render_template ('explore.html', \n",
    "                           statement = statement,\n",
    "                           statement_sentiment = statement_sentiment,\n",
    "                           replacement_ners = replacement_ners,\n",
    "                           replacement_verbs = replacement_verbs,\n",
    "                           replacement_verbphrases = replacement_verbphrases,\n",
    "                           replacement_nouns = replacement_nouns,\n",
    "                           replacement_nounphrases = replacement_nounphrases,\n",
    "                           replacement_adjectives = replacement_adjectives,\n",
    "                           replacement_adjphrases = replacement_adjphrases,\n",
    "                           ner_categories = ner_categories)\n",
    "    #return redirect(url_for('main', statement = _statement))\n",
    " \n",
    "    # validate the received values\n",
    "    # if _statement:\n",
    "        # return json.dumps({'html':'<span>All fields good !!</span>'})\n",
    "    # else:\n",
    "        # return json.dumps({'html':'<span>Enter the required fields</span>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/generate',methods=['POST'])\n",
    "def generate():\n",
    "    print(\"Generating alternatives\")\n",
    "    statement = request.form['stmnt']\n",
    "    alt = []\n",
    "    alt_stmnts = []\n",
    "    for i in request.form:\n",
    "        if i != \"stmnt\": \n",
    "            opt = request.form.getlist(i)\n",
    "            for j in opt:\n",
    "                if i != j:\n",
    "                    alt.append((i, j))\n",
    "            \n",
    "#        statement = statement.replace(i, request.form[i])\n",
    "#    print (statement)\n",
    "    from itertools import chain, combinations\n",
    "    def all_subsets(ss):\n",
    "        return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))\n",
    "    \n",
    "    asubs = set(all_subsets(alt))\n",
    "    print(len(asubs)) \n",
    "    for subset in asubs:\n",
    "        nustat = statement\n",
    "        for i in subset:\n",
    "            nustat = nustat.replace(i[0],i[1])\n",
    "        alt_stmnts.append( (nustat, sentiments(nustat, nlp)) ) \n",
    "    \n",
    "    return render_template ('generate.html', \n",
    "                       statements = set(alt_stmnts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from werkzeug.serving import run_simple\n",
    "    app.debug = True\n",
    "    run_simple('localhost', 2000, app, use_debugger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587 texts collected.\n",
      "Training on 1,121,604 character sequences.\n",
      "Epoch 1/1\n",
      "8762/8762 [==============================] - 4768s 544ms/step - loss: 1.3599\n",
      "####################\n",
      "Temperature: 0.2\n",
      "####################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "####################\n",
      "Temperature: 0.5\n",
      "####################\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The email account of the decryptent of ends to the service breach is no malware and it is the time of the training to the employee of the attackers are sendities to the service and districted accounts to see the service to determine an email online and the indictment of the server. The common medi\n",
      "\n",
      "####################\n",
      "Temperature: 1.0\n",
      "####################\n",
      "\n",
      "\n",
      "Neew.” Ransoma Wennins ultiminations and datid on computers that access to the shows had steal attaching and cybertatics is that they call, sequalt swiples, it allergely about a federal fine to correct conmitting email attachments individuals have bitcoinal, the forming wins developing data, and t\n",
      "\n",
      "3 Buts-generation password.They would see order-entire data this and that thief-security. The defense computer that they are tried becove that opt stolen files cost, as voter to as being a minimum credentious press. Multinal, etforcerlisd files came complicence to incluist its additional point spa\n",
      "\n",
      "North Carolina State University scam delivered the attack in the service stating as every personal indictment when the emails can do the service in the party of a study start security to do the document downloading a threat to a spam email and searcheus compromises services and access to the techn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "textgen = textgenrnn()\n",
    "textgen.train_from_file('/Users/shrey/AnacondaProjects/Application_reviews/Code/caltagirone.txt', num_epochs=1)\n",
    "textgen.save('caltagirone.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian Hackers Pokora account and that it is to data.”[17] There may hall indictming their proteminals or the missages also healthcare underancemented May 19 movested his likelie to a web filter in enlided their web shows being gained to as  in when they were adveneed the top time to deal stalming to the WikiLeaks.After the scale were received for yet that other seconds from the malicious society that as one that commit downloads such as Rvanyan was one via an document.The Idelor evot works f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textgen.generate(prefix=\"Russian Hackers\", temperature=1, max_gen_length=500  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module textgenrnn.textgenrnn:\n",
      "\n",
      "generate(n=1, return_as_list=False, prefix=None, temperature=0.5, max_gen_length=300, interactive=False, top_n=3) method of textgenrnn.textgenrnn.textgenrnn instance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(textgen.generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen.save('russian_hackers.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment with wordnet for replacing verbs, nouns, and adjectives\n",
    "## Not yet used.\n",
    "#    replacement_verbs_synonyms = []\n",
    "#    replacement_verbs_antonyms = []\n",
    "#    replacement_adjectives_synonyms = []\n",
    "#    replacement_adjectives_antonyms = []\n",
    "#    replacement_nouns_synonyms = []\n",
    "#    replacement_nouns_antonyms = [] \n",
    "#    for verb in to_replace_verbs:\n",
    "#        for syn in wordnet.synsets(verb): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_verbs_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#\t\t\t\t    #replacement_verbs_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_verbs_antonyms.append(m.name())\n",
    "#\n",
    "#    print('Replacement Verb Synonyms', set(replacement_verbs_synonyms))\n",
    "#    print('Replacement Verb Antonyms', set(replacement_verbs_antonyms))\n",
    "#\n",
    "#\t\n",
    "#    for noun in to_replace_nouns:\n",
    "#        for syn in wordnet.synsets(noun): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_nouns_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#                    #replacement_nouns_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_nouns_antonyms.append(m.name())\n",
    "#\t\t\t\t\t\n",
    "#    print('Replacement Noun Synonyms', set(replacement_nouns_synonyms))\n",
    "#    print('Replacement Noun Antonyms', set(replacement_nouns_antonyms))\t\t\n",
    "#\n",
    "#\t\n",
    "#    for adjective in to_replace_adjectives:\n",
    "#        for syn in wordnet.synsets(adjective): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_adjectives_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#                    #replacement_adjectives_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_adjectives_antonyms.append(m.name(\n",
    "#\t\t\t\t\t\n",
    "#    print('Replacement Adj Synonyms', set(replacement_adjectives_synonyms))\n",
    "#    print('Replacement Adj Antonyms', set(replacement_adjectives_antonyms))\t\n",
    "\n",
    "#    nlp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
