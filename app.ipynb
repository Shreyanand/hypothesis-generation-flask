{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shrey/AnacondaProjects/stanford-corenlp-full-2018-02-27\n",
      "Word 2 Vec model ready\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "app = Flask(__name__)\n",
    "from nltk.corpus import wordnet\n",
    "import pymysql\n",
    "import configparser\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import gensim\n",
    "import json\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"config.txt\")\n",
    "word2vec_path = config.get(\"configuration\",\"word2vec_path\")\n",
    "stanford_corenlp_path = config.get(\"configuration\",\"stanford_corenlp_path\")\n",
    "print(stanford_corenlp_path)\n",
    "\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "# Connect to DB\n",
    "mydbhost = config.get(\"configuration\",\"mydbhost\")\n",
    "mydbuser = config.get(\"configuration\",\"mydbuser\")\n",
    "mydbpasswd = config.get(\"configuration\",\"mydbpasswd\")\n",
    "mydbdb = config.get(\"configuration\",\"mydbdb\")\n",
    "\n",
    "# Importing word2vec to find similarity and neighboring words\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True, limit=500000) \n",
    "print('Word 2 Vec model ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiments(word, nlp):\n",
    "    \"\"\" This function calls Stanford Core Nlp's sentiment annotator\n",
    "    \n",
    "        Args:\n",
    "            word (String): The input word or sentence for setiment detection\n",
    "            nlp (StanfordCoreNLP): Core nlp instance for sentiment detection\n",
    "        Returns:\n",
    "            String : Sentiment value: Very negative; Negative; Neutral; \n",
    "            postive; Very positive\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sjson = nlp.annotate(word,properties={'annotators': 'sentiment','outputFormat': 'json','timeout': 1000})\n",
    "        res = json.loads(sjson)\n",
    "        sval = res[\"sentences\"][0][\"sentiment\"]\n",
    "        return(sval)\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ners(sentence, nlp):\n",
    "    sentence_ner = nlp.ner(sentence + ' dummy')\n",
    "    to_replace_ners = []\n",
    "    org = ''\n",
    "    kind = ''\n",
    "    for (i, j) in sentence_ner: #i = NCSU, j = ORGANISATION; i = bent, j = 0\n",
    "        if(j == 'O'):\n",
    "            if org != '': \n",
    "                to_replace_ners.append((org[1:], kind))\n",
    "                org = ''\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            org = org + \"_\" + i\n",
    "            kind = j\n",
    "    #print (sentence_ner)   \n",
    "    return to_replace_ners\n",
    "\n",
    "#ners(\"Barrack Obama went to the Harvard University\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tags(statement, nlp):\n",
    "    \n",
    "    sentence_tags = nlp.pos_tag(statement)\n",
    "    \n",
    "    to_replace_verbs = []\n",
    "    to_replace_verbphrases = []\n",
    "    to_replace_adjectives = []\n",
    "    to_replace_adjphrases = []\n",
    "    to_replace_nouns = []\n",
    "    to_replace_nounphrases = []\n",
    "    \n",
    "    verb_check = 0\n",
    "    noun_check = 0\n",
    "    adj_check = 0\n",
    "\n",
    "    for (i, j) in sentence_tags: # Here verb checks are activated after a verb is discovered, so pair of verbs as verbphrases? Check Logic.\n",
    "        \n",
    "        \n",
    "      if(verb_check == 1):\n",
    "        verbphrase = verb + '_' + i\n",
    "        to_replace_verbphrases.append((verbphrase, i))\n",
    "        verb_check = 0 \n",
    "\n",
    "      if(noun_check == 1):\n",
    "        nounphrase = noun + '_' + i\n",
    "        to_replace_nounphrases.append((nounphrase, i))\n",
    "        noun_check = 0 \n",
    "\n",
    "      if(adj_check == 1):\n",
    "        adjphrase = adj + '_' + i\n",
    "        to_replace_adjphrases.append((adjphrase, i))\n",
    "        adj_check = 0 \n",
    "\n",
    "      if(j == 'VBD' or j=='VBZ' or j == 'VBP' or j == 'VBN' or j == 'VBG' or j == 'VB'):\n",
    "        to_replace_verbs.append(i)\n",
    "        verb = i\n",
    "        verb_check = 1\n",
    "\n",
    "      if(j == 'NN' or j=='NNS' or j == 'NNP' or j == 'NNPS'):\n",
    "        to_replace_nouns.append(i)\n",
    "        noun = i\n",
    "        noun_check = 1\n",
    "\n",
    "      if(j == 'JJ'):\n",
    "        to_replace_adjectives.append(i)\n",
    "        adj = i\n",
    "        adj_check = 1\n",
    "        \n",
    "    return (to_replace_verbs, \n",
    "            to_replace_verbphrases,\n",
    "            to_replace_adjectives,\n",
    "            to_replace_adjphrases,\n",
    "            to_replace_nouns,\n",
    "            to_replace_nounphrases)\n",
    "\n",
    "#pos_tags(\"Barrack Obama went to the Harvard University\", nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_similar(to_replace_ners, to_replace_verbs, to_replace_verbphrases,\n",
    "    to_replace_adjectives, to_replace_adjphrases,\n",
    "    to_replace_nouns, to_replace_nounphrases,  model):\n",
    "    \n",
    "    ## These are using W2V, currently used.\n",
    "    topk = 10\n",
    "    replacement_ners = []\n",
    "    replacement_verbs = []\n",
    "    replacement_verbphrases = []\n",
    "    replacement_nouns = []\n",
    "    replacement_nounphrases = []\n",
    "    replacement_adjectives = []\n",
    "    replacement_adjphrases = []\n",
    "    \n",
    "    for (i, j) in to_replace_ners:\n",
    "        try:\n",
    "            similar_ners = model.most_similar([i, j.lower()], [], topk)\n",
    "            senti_ners = []\n",
    "            for (similar_ner, score) in similar_ners:\n",
    "                senti_ners.append([similar_ner, score, sentiments(similar_ner, nlp)])\n",
    "            replacement_ners.append((i, senti_ners))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "        \n",
    "        ## Use W2V for replacing nouns, verbs, adj verbphrases, nounphrases, and adj phrases\n",
    "    \n",
    "    for verb in to_replace_verbs:\n",
    "        try: \n",
    "            similar_verbs = model.most_similar(verb, [], topk)\n",
    "            senti_verbs = [[i, j, sentiments(i, nlp)] for (i, j) in similar_verbs]\n",
    "            replacement_verbs.append((verb, senti_verbs))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "            \n",
    "    for (verbphrase, nn) in to_replace_verbphrases:\n",
    "        try:\n",
    "            similar_verbphrases = model.most_similar([verbphrase, nn], [], topk)\n",
    "            replacement_verbphrases.append((verbphrase, similar_verbphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    " \n",
    "    for noun in to_replace_nouns:\n",
    "        try:\n",
    "            similar_nouns = model.most_similar(noun, [], topk)\n",
    "            senti_nouns = [[i, j, sentiments(i, nlp)] for (i, j) in similar_nouns]\n",
    "            replacement_nouns.append((noun, senti_nouns))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for (nounphrase, nn) in to_replace_nounphrases:\n",
    "        try:\n",
    "            similar_nounphrases = model.most_similar([nounphrase, nn], [], topk)\n",
    "            replacement_nounphrases.append((nounphrase, similar_nounphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for adjective in to_replace_adjectives: \n",
    "        try: \n",
    "            similar_adjectives = model.most_similar(adjective, [], topk)\n",
    "            senti_adjectives = [[i, j, sentiments(i, nlp)] for (i, j) in similar_adjectives]\n",
    "            replacement_adjectives.append((adjective, senti_adjectives))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "        \n",
    "    \n",
    "    \n",
    "    for (adjphrase, nn) in to_replace_adjphrases:\n",
    "        try:\n",
    "            similar_adjphrases = model.most_similar([adjphrase, nn], [], topk)\n",
    "            replacement_adjphrases.append((adjphrase, similar_adjphrases))\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "    \n",
    "#     print(\"Alternative NERs\" + str(replacement_ners))\n",
    "#     print(\"Alternative Verbs\" +  str(replacement_verbs))\n",
    "#     print(\"Alternative Verb Phrases\" + str(replacement_verbphrases))\n",
    "#     print(\"Alternative Noun\" + str(replacement_nouns))\n",
    "#     print(\"Alternative Noun Phrases\" + str(replacement_nounphrases))\n",
    "#     print(\"Alternative Adjectives\" + str(replacement_adjectives))\n",
    "#     print(\"Alternative Adjective Phrases\" + str(replacement_adjphrases))\n",
    "    \n",
    "    \n",
    "    return (replacement_ners,\n",
    "            replacement_verbs,\n",
    "            replacement_verbphrases,\n",
    "            replacement_nouns,\n",
    "            replacement_nounphrases,\n",
    "            replacement_adjectives,\n",
    "            replacement_adjphrases)\n",
    "\n",
    "# statement = \"Barrack Obama went to the Harvard University\" \n",
    "# to_replace_verbs, to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases, to_replace_nouns, to_replace_nounphrases = pos_tags(\n",
    "#     statement, nlp)\n",
    "# w2v_similar(ners(statement, nlp), to_replace_verbs, \n",
    "#             to_replace_verbphrases,\n",
    "#             to_replace_adjectives,\n",
    "#             to_replace_adjphrases,\n",
    "#             to_replace_nouns,\n",
    "#             to_replace_nounphrases, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to Wikipedia Database of instance and class. \n",
    "## For each ner, find similar ners using W2V and append to replacement_ners (earlier version)\n",
    "## For each similar ner, find it's category from simple types Wikipedia Database\n",
    "def wiki_ontology(to_replace_ners):\n",
    "    #print(to_replace_ners)\n",
    "    ner_categories = []\n",
    "    \n",
    "    for ner in to_replace_ners:\n",
    "        replace = []\n",
    "        connection = pymysql.connect(host=mydbhost, user=mydbuser, passwd=mydbpasswd, db=mydbdb)\n",
    "        ne = ner[0]\n",
    "        print(ne)\n",
    "        try:\n",
    "            with connection.cursor() as cursor:\n",
    "            # Execute SQL select statement\n",
    "                query = \"SELECT instance, class FROM simple_types where instance like CONCAT('%', '\"+ ne + \"' ,'%')\"\n",
    "                cursor.execute(query)\n",
    "                # Get the number of rows in the resultset\n",
    "                numrows = cursor.rowcount\n",
    "                # Get and display one row at a time\n",
    "                rep = []\n",
    "                for x in range(0, 10): #top 10 results from wikicat\n",
    "                    row = cursor.fetchone()\n",
    "                    if row:\n",
    "                        supernym = row[1][9:-1].replace('_', ' ')\n",
    "                        rep.append([supernym, 0, 0])\n",
    "                if rep:\n",
    "                    ner_categories.append((ne.replace('_', ' '), rep))\n",
    "        finally:\n",
    "          # Close connection.\n",
    "          connection.close()\n",
    "                    \n",
    "    return (ner_categories)\n",
    "\n",
    "# to_replace_nlp = ners(\"Obama went to the Harvard University\", nlp)\n",
    "# print(to_replace_nlp)\n",
    "# wiki_ontology(to_replace_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'subject': 'terrorists', 'subjectSpan': [1, 2], 'relation': 'attack', 'relationSpan': [5, 6], 'object': 'United States office building', 'objectSpan': [7, 11]}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['terrorists'], ['United States office building'])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def openie(statement, nlp):\n",
    "    # output = nlp2.annotate(s, properties={\"annotators\":\"openie\", \"outputFormat\": \"json\",  \"openie.triple.strict\":\"false\",\n",
    "    #                                        })\n",
    "    to_replace_sub = []\n",
    "    to_replace_obj = []\n",
    "    output = nlp.annotate(s, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\", \"outputFormat\": \"json\"})\n",
    "    result = [json.loads(output)[\"sentences\"][0][\"openie\"] for item in output][0][0]\n",
    "    to_replace_sub.append(result['subject'])\n",
    "    to_replace_obj.append(result['object'])\n",
    "    return to_replace_sub, to_replace_obj\n",
    "\n",
    "# from pycorenlp import StanfordCoreNLP as SCNLP\n",
    "# nlp2 = SCNLP(\"http://localhost:9000\")\n",
    "statement = \"The terrorists are planning to attack a United States office building.\"\n",
    "openie(statement, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_to_specific(to_replace_sub, to_replace_obj):\n",
    "    \n",
    "    def get_query(words):\n",
    "        query = \"\"\"SELECT instance, class FROM simple_types where \n",
    "        class like CONCAT('%', '\"\"\"+ words[0]\n",
    "        l = len(words)\n",
    "        if l > 1:\n",
    "            for i in range(1, l):\n",
    "                query = query + \"\"\"' ,'%')\n",
    "                AND class like CONCAT('%', '\"\"\"+ words[i]\n",
    "        query = query + \"\"\"' ,'%')\"\"\"\n",
    "        return query\n",
    "\n",
    "    connection = pymysql.connect(host=mydbhost, user=mydbuser, passwd=mydbpasswd, db=mydbdb)\n",
    "    replacement_sub = []\n",
    "    replacement_obj = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        for entity in to_replace_sub:\n",
    "            ne = entity.split(' ')\n",
    "            query = get_query(ne)\n",
    "            print(query)\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                numrows = cursor.rowcount\n",
    "                rep = []\n",
    "                for x in range(0, 20): #top 20 results from wikicat\n",
    "                    row = cursor.fetchone()\n",
    "                    if row:\n",
    "                          rep.append(row[0][1:-1].replace('_', ' '))\n",
    "                if rep:\n",
    "                    print(rep)\n",
    "                    replacement_sub.append((entity, rep))\n",
    "        print(replacement_sub)\n",
    "        for entity in to_replace_obj:\n",
    "            ne = entity.split(' ')\n",
    "            query = get_query(ne)\n",
    "            print(query)\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(query)\n",
    "                numrows = cursor.rowcount\n",
    "                rep = []\n",
    "                for x in range(0, 20): #top 20 results from wikicat\n",
    "                    row = cursor.fetchone()\n",
    "                    if row:\n",
    "                          rep.append(row[0][1:-1].replace('_', ' '))\n",
    "                if rep:\n",
    "                    replacement_obj.append((entity, rep))\n",
    "        print(replacement_obj)\n",
    "    finally:\n",
    "        connection.close()\n",
    "            \n",
    "    return (replacement_sub, replacement_obj)\n",
    "    \n",
    "\n",
    "# from pycorenlp import StanfordCoreNLP as SCNLP\n",
    "# nlp2 = SCNLP(\"http://localhost:9000\")\n",
    "# statement = \"The terrorists are planning to attack a United States office building.\"\n",
    "# to_replace_sub, to_replace_obj = openie(statement, nlp2)\n",
    "# print(general_to_specific(to_replace_sub, to_replace_obj))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/\")\n",
    "def main():\n",
    "    print('Main Page opened')\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/explore',methods=['POST'])\n",
    "def explore():   \n",
    "    # importing StandfordCoreNLP to tokenize, tag, and ner\n",
    "    # Tree syntax of natural language: http://www.cs.cornell.edu/courses/cs474/2004fa/lec1.pdf\n",
    "\n",
    "\n",
    "    # print(\"In Explore\")\n",
    "    # print(\"StanfordCoreNLP path:\", stanford_corenlp_path)\n",
    "    \n",
    "\n",
    "    # read the posted values from the UI\n",
    "    statement = ''\n",
    "    print('explore')\n",
    "    if request.form.get('inputStatement', None):\n",
    "        statement = request.form['inputStatement']\n",
    "        print(statement)\n",
    "    \n",
    "    \n",
    "    statement_sentiment = \"Sentiment of the query is: \" + sentiments(statement, nlp)\n",
    "    to_replace_ners = ners(statement, nlp)\n",
    "    print(statement_sentiment)\n",
    "    #sentence_tokens = nlp.word_tokenize(statement)\n",
    "    # sentence_parse = nlp.parse(statement)\n",
    "    # sentence_dependency = nlp.dependency_parse(statement)\n",
    "\n",
    "    to_replace_verbs, to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases, to_replace_nouns, to_replace_nounphrases = pos_tags(statement, nlp)\n",
    "    \n",
    "    replacement_ners, replacement_verbs, replacement_verbphrases, replacement_nouns, replacement_nounphrases, replacement_adjectives, replacement_adjphrases = w2v_similar(\n",
    "        to_replace_ners, to_replace_verbs, \n",
    "        to_replace_verbphrases, to_replace_adjectives, to_replace_adjphrases,\n",
    "        to_replace_nouns, to_replace_nounphrases, model)\n",
    "    \n",
    "    replacement_ners.extend(wiki_ontology(to_replace_ners))\n",
    "    \n",
    "    print(\"Analysis complete\")\n",
    "    return render_template ('explore.html', \n",
    "                           statement = statement,\n",
    "                           statement_sentiment = statement_sentiment,\n",
    "                           replacement_ners = replacement_ners,\n",
    "                           replacement_verbs = replacement_verbs,\n",
    "                           replacement_verbphrases = replacement_verbphrases,\n",
    "                           replacement_nouns = replacement_nouns,\n",
    "                           replacement_nounphrases = replacement_nounphrases,\n",
    "                           replacement_adjectives = replacement_adjectives,\n",
    "                           replacement_adjphrases = replacement_adjphrases,\n",
    "                           )\n",
    "    #return redirect(url_for('main', statement = _statement))\n",
    " \n",
    "    # validate the received values\n",
    "    # if _statement:\n",
    "        # return json.dumps({'html':'<span>All fields good !!</span>'})\n",
    "    # else:\n",
    "        # return json.dumps({'html':'<span>Enter the required fields</span>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/generate',methods=['POST'])\n",
    "def generate():\n",
    "    print(\"Generating alternatives\")\n",
    "    statement = request.form['stmnt']\n",
    "    alt = []\n",
    "    alt_stmnts = []\n",
    "    for i in request.form:\n",
    "        if i != \"stmnt\": \n",
    "            opt = request.form.getlist(i)\n",
    "            for j in opt:\n",
    "                if i != j:\n",
    "                    alt.append((i, j))\n",
    "            \n",
    "#        statement = statement.replace(i, request.form[i])\n",
    "#    print (statement)\n",
    "    from itertools import chain, combinations\n",
    "    def all_subsets(ss):\n",
    "        return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))\n",
    "    \n",
    "    asubs = set(all_subsets(alt))\n",
    "    print(len(asubs)) \n",
    "    for subset in asubs:\n",
    "        nustat = statement\n",
    "        for i in subset:\n",
    "            nustat = nustat.replace(i[0],i[1])\n",
    "        alt_stmnts.append( (nustat, sentiments(nustat, nlp)) ) \n",
    "    \n",
    "    return render_template ('generate.html', \n",
    "                       statements = set(alt_stmnts))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core NLP instance\n",
      "explore\n",
      "The terrorists are planning to attack the building. \n",
      "Sentiment of the query is: Negative\n",
      "\"word 'are_planning' not in vocabulary\"\n",
      "\"word 'planning_to' not in vocabulary\"\n",
      "\"word 'attack_the' not in vocabulary\"\n",
      "\"word 'terrorists_are' not in vocabulary\"\n",
      "\"word 'building_.' not in vocabulary\"\n",
      "Analysis complete\n"
     ]
    }
   ],
   "source": [
    "nlp = StanfordCoreNLP(\"http://localhost\", 9000)\n",
    "print('Core NLP instance')\n",
    "if __name__ == \"__main__\":\n",
    "    from werkzeug.serving import run_simple\n",
    "    app.debug = True\n",
    "    run_simple('localhost', 2000, app, use_debugger=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment with wordnet for replacing verbs, nouns, and adjectives\n",
    "## Not yet used.\n",
    "#    replacement_verbs_synonyms = []\n",
    "#    replacement_verbs_antonyms = []\n",
    "#    replacement_adjectives_synonyms = []\n",
    "#    replacement_adjectives_antonyms = []\n",
    "#    replacement_nouns_synonyms = []\n",
    "#    replacement_nouns_antonyms = [] \n",
    "#    for verb in to_replace_verbs:\n",
    "#        for syn in wordnet.synsets(verb): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_verbs_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#\t\t\t\t    #replacement_verbs_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_verbs_antonyms.append(m.name())\n",
    "#\n",
    "#    print('Replacement Verb Synonyms', set(replacement_verbs_synonyms))\n",
    "#    print('Replacement Verb Antonyms', set(replacement_verbs_antonyms))\n",
    "#\n",
    "#\t\n",
    "#    for noun in to_replace_nouns:\n",
    "#        for syn in wordnet.synsets(noun): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_nouns_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#                    #replacement_nouns_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_nouns_antonyms.append(m.name())\n",
    "#\t\t\t\t\t\n",
    "#    print('Replacement Noun Synonyms', set(replacement_nouns_synonyms))\n",
    "#    print('Replacement Noun Antonyms', set(replacement_nouns_antonyms))\t\t\n",
    "#\n",
    "#\t\n",
    "#    for adjective in to_replace_adjectives:\n",
    "#        for syn in wordnet.synsets(adjective): \n",
    "#            for l in syn.lemmas():\n",
    "#                replacement_adjectives_synonyms.append(l.name())\n",
    "#                if l.antonyms():\n",
    "#                    #print('L.Antonyms:', l.antonyms())\n",
    "#                    #replacement_adjectives_antonyms.append(l.antonyms()[0].name())\n",
    "#                    for m in l.antonyms():\n",
    "#                        replacement_adjectives_antonyms.append(m.name(\n",
    "#\t\t\t\t\t\n",
    "#    print('Replacement Adj Synonyms', set(replacement_adjectives_synonyms))\n",
    "#    print('Replacement Adj Antonyms', set(replacement_adjectives_antonyms))\t\n",
    "\n",
    "#    nlp.close()\n",
    "\n",
    "#Examples: Russian Hackers sent phishing emails to North Carolina State University servers\n",
    "#Examples:  Through a HTTP message sent to host, the malware begins to proxy TCP connections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
